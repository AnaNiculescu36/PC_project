## Task 1
#' Function thath takes as input argument the subset of the relevant variables
#' and the variable to predict. This function returns the maximum likelihood estimator.
#' This estimator is given
#' by a Newton-Raphson algorithm (or any other algorithm of optimization).
my_data = read.csv("http://spreadsheets.google.com/pub?key=0AnypY27pPCJydC1vRVEzM1VJQnNneFo5dWNzR1F5Umc&output=csv", header = TRUE)
exam_y = read.csv("ex4y.dat")
exam_y = read.csv("data/ex4y.dat")
exam_y
exam_x = read.csv("data/ex4x.dat")
View(exam_x)
## Task 1
#' Function thath takes as input argument the subset of the relevant variables
#' and the variable to predict. This function returns the maximum likelihood estimator.
#' This estimator is given
#' by a Newton-Raphson algorithm (or any other algorithm of optimization).
exam_x = read.csv("data/ex4x.dat", sep ="\t")
View(exam_x)
## Task 1
#' Function thath takes as input argument the subset of the relevant variables
#' and the variable to predict. This function returns the maximum likelihood estimator.
#' This estimator is given
#' by a Newton-Raphson algorithm (or any other algorithm of optimization).
exam_x = read.delim("data/ex4x.dat", sep ="\t")
View(exam_x)
View(exam_x)
## Task 1
#' Function thath takes as input argument the subset of the relevant variables
#' and the variable to predict. This function returns the maximum likelihood estimator.
#' This estimator is given
#' by a Newton-Raphson algorithm (or any other algorithm of optimization).
exam_x = read.delim("data/ex4x.dat", sep ="\t")
## Task 1
#' Function thath takes as input argument the subset of the relevant variables
#' and the variable to predict. This function returns the maximum likelihood estimator.
#' This estimator is given
#' by a Newton-Raphson algorithm (or any other algorithm of optimization).
exam_x = read.delim("data/ex4x.dat", sep =" ")
## Task 1
#' Function thath takes as input argument the subset of the relevant variables
#' and the variable to predict. This function returns the maximum likelihood estimator.
#' This estimator is given
#' by a Newton-Raphson algorithm (or any other algorithm of optimization).
exam_x = read.delim("data/ex4x.dat", sep ="   ")
## Task 1
#' Function thath takes as input argument the subset of the relevant variables
#' and the variable to predict. This function returns the maximum likelihood estimator.
#' This estimator is given
#' by a Newton-Raphson algorithm (or any other algorithm of optimization).
exam_x = read.delim("data/ex4x.dat", sep =" ")
## Task 1
#' Function thath takes as input argument the subset of the relevant variables
#' and the variable to predict. This function returns the maximum likelihood estimator.
#' This estimator is given
#' by a Newton-Raphson algorithm (or any other algorithm of optimization).
exam_x = read.delim("data/ex4x.dat", sep = " ")
exam_y = read.delim("data/ex4y.dat", sep = " ")
exam_y = read.delim("data/ex4y.dat")
library(dplyr)
## Task 1
#' Function thath takes as input argument the subset of the relevant variables
#' and the variable to predict. This function returns the maximum likelihood estimator.
#' This estimator is given
#' by a Newton-Raphson algorithm (or any other algorithm of optimization).
exam_x = read.delim("data/ex4x.dat", sep = " ")
exam_y = read.delim("data/ex4y.dat")
# cleaning the data
exam_x <- exam_x[c(4, 7), ]
colnames(exam_x) <- c("x1", "x2")
colnames(exam_y) <- "y"
exam_x[c(4, 7), ]
exam_x = read.delim("data/ex4x.dat", sep = " ")
exam_x
library(dplyr)
## Task 1
#' Function thath takes as input argument the subset of the relevant variables
#' and the variable to predict. This function returns the maximum likelihood estimator.
#' This estimator is given
#' by a Newton-Raphson algorithm (or any other algorithm of optimization).
exam_x = read.delim("data/ex4x.dat", sep = " ")
exam_y = read.delim("data/ex4y.dat")
# cleaning the data
exam_x <- exam_x[ ,c(4, 7)]
colnames(exam_x) <- c("x1", "x2")
colnames(exam_y) <- "y"
exam_x
knitr::opts_chunk$set(echo = TRUE)
exam_x = read.delim("data/ex4x.dat", sep = " ")
exam_y = read.delim("data/ex4y.dat")
# cleaning the data
exam_x <- exam_x[ ,c(4, 7)]
colnames(exam_x) <- c("x1", "x2")
colnames(exam_y) <- "y"
basic.mle <- function(predictors, predicted) {
}
exam_x = read.delim("data/ex4x.dat", sep = " ")
exam_y = read.delim("data/ex4y.dat")
# cleaning the data
exam_x <- exam_x[ ,c(4, 7)]
colnames(exam_x) <- c("x1", "x2")
colnames(exam_y) <- "y"
basic.mle <- function(predictors, predicted) {
}
exam_df <- bindcols(exam_x, exam_y)
exam_df <- cbind(exam_x, exam_y)
exam_x = read.delim("data/ex4x.dat", sep = " ")
exam_y = read.delim("data/ex4y.dat")
# cleaning the data
exam_x <- exam_x[ ,c(4, 7)]
colnames(exam_x) <- c("test1", "test2")
colnames(exam_y) <- "admitted"
exam_df <- cbind(exam_x, exam_y)
exam_x = read.delim("data/ex4x.dat", sep = " ")
exam_y = read.delim("data/ex4y.dat")
# cleaning the data
exam_x <- exam_x[ ,c(4, 7)]
colnames(exam_x) <- c("test1", "test2")
colnames(exam_y) <- "admitted"
exam_df <- cbind(exam_x, exam_y)
library(ggplot)
library(ggplot2)
# cleaning the data
exam_x <- exam_x[ ,c(4, 7)]
ggplot2::ggplot(exam_df, ggplot2::aes(x = test1, y = test_2, col = admitted))
ggplot2::ggplot(exam_df, ggplot2::aes(x = test1, y = test2, col = admitted))
library(ggplot2)
exam_x = read.delim("data/ex4x.dat", sep = " ")
exam_y = read.delim("data/ex4y.dat")
# cleaning the data
exam_x <- exam_x[ ,c(4, 7)]
colnames(exam_x) <- c("test1", "test2")
colnames(exam_y) <- "admitted"
exam_df <- cbind(exam_x, exam_y)
t
ggplot2::ggplot(exam_df, ggplot2::aes(x = test1, y = test2, col = admitted))
View(exam_x)
exam_x = read.delim("data/ex4x.dat", sep = " ")
exam_y = read.delim("data/ex4y.dat")
# cleaning the data
exam_x <- exam_x[ ,c(4, 7)]
colnames(exam_x) <- c("test1", "test2")
colnames(exam_y) <- "admitted"
exam_df <- dplyr::bind_cols(exam_x, exam_y)
ggplot2::ggplot(exam_df, ggplot2::aes(x = test1, y = test2, col = admitted))
View(exam_df)
ggplot2::ggplot(exam_df, ggplot2::aes(x = test1, y = test2))
ggplot2::ggplot(exam_df, ggplot2::aes(x = test1, y = test2, col = admitted)) +
ggplot2::geom_point()
ggplot2::ggplot(exam_df, ggplot2::aes(x = test1, y = test2, col = as.factor(admitted))) +
ggplot2::geom_point()
numeric(3)
# derivative of J (gradient)
grad = function (x,y,th,m) {
return( 1/m * t(x) %*% (h(x,th) - y))
} # grad(x,y,th,m)
# sigmoid
g = function (z) {
return (1 / (1 + exp(-z) ))
} # plot(g(c(1,2,3,4,5,6)))
# hypothesis
h = function (x,th) {
return( g(x %*% th) )
} # h(x,th)
# cost
J = function (x,y,th,m) {
return( 1/m * sum(-y * log(h(x,th)) - (1 - y) * log(1 - h(x,th))) )
} # J(x,y,th,m)
# derivative of J (gradient)
grad = function (x,y,th,m) {
return( 1/m * t(x) %*% (h(x,th) - y))
} # grad(x,y,th,m)
# Hessian
H = function (x,y,th,m) {
return (1/m * t(x) %*% x * diag(h(x,th)) * diag(1 - h(x,th)))
} # H(x,y,th,m)
basic.mle <- function(x, y) {
j = array(0, c(10,1))
m = length(x)
x = matrix(c(rep(1, m), x$test1, x$test2), ncol=3)
y = matrix(x$admitted, ncol=1)
th = matrix(0,3)
for (i in 1:100) {
j[i] = J(x,y,th,m) # stores each iteration Cost
th = th - solve(H(x,y,th,m)) %*% grad(x,y,th,m)
}
}
j <- basic.mle(exam_x, exam_y)
y = matrix(y$admitted, ncol=1)
th = th - solve(H(x,y_calc,th,m)) %*% grad(x,y_calc,th,m)
# sigmoid
g = function (z) {
return (1 / (1 + exp(-z) ))
} # plot(g(c(1,2,3,4,5,6)))
# hypothesis
h = function (x,th) {
return( g(x %*% th) )
} # h(x,th)
# cost
J = function (x,y,th,m) {
return( 1/m * sum(-y * log(h(x,th)) - (1 - y) * log(1 - h(x,th))) )
} # J(x,y,th,m)
# derivative of J (gradient)
grad = function (x,y,th,m) {
return( 1/m * t(x) %*% (h(x,th) - y))
} # grad(x,y,th,m)
# Hessian
H = function (x,y,th,m) {
return (1/m * t(x) %*% x * diag(h(x,th)) * diag(1 - h(x,th)))
} # H(x,y,th,m)
basic.mle <- function(x, y) {
j = array(0, c(10,1))
m = length(x)
x = matrix(c(rep(1, m), x$test1, x$test2), ncol=3)
y_calc = matrix(y$admitted, ncol=1)
th = matrix(0,3)
for (i in 1:100) {
j[i] = J(x,y,th,m) # stores each iteration Cost
th = th - solve(H(x,y_calc,th,m)) %*% grad(x,y_calc,th,m)
}
j
}
j <- basic.mle(exam_x, exam_y)
# sigmoid
g = function (z) {
return (1 / (1 + exp(-z) ))
} # plot(g(c(1,2,3,4,5,6)))
# hypothesis
h = function (x,th) {
return( g(x %*% th) )
} # h(x,th)
# cost
J = function (x,y,th,m) {
return( 1/m * sum(-y * log(h(x,th)) - (1 - y) * log(1 - h(x,th))) )
} # J(x,y,th,m)
# derivative of J (gradient)
grad = function (x,y,th,m) {
return( 1/m * t(x) %*% (h(x,th) - y))
} # grad(x,y,th,m)
# Hessian
H = function (x,y,th,m) {
return (1/m * t(x) %*% x * diag(h(x,th)) * diag(1 - h(x,th)))
} # H(x,y,th,m)
basic.mle <- function(x, y) {
j = array(0, c(10,1))
m = length(x)
x = matrix(c(rep(1, m), x$test1, x$test2), ncol=3)
y_calc = matrix(y$admitted, ncol=1)
th = matrix(0,3)
for (i in 1:100) {
j[i] = J(x, y, th, m) # stores each iteration Cost
th = th - solve(H(x,y_calc,th,m)) %*% grad(x,y_calc,th,m)
}
j
}
j <- basic.mle(exam_x, exam_y)
m = length(x$test1)
# sigmoid
g = function (z) {
return (1 / (1 + exp(-z) ))
} # plot(g(c(1,2,3,4,5,6)))
# hypothesis
h = function (x,th) {
return( g(x %*% th) )
} # h(x,th)
# cost
J = function (x,y,th,m) {
return( 1/m * sum(-y * log(h(x,th)) - (1 - y) * log(1 - h(x,th))) )
} # J(x,y,th,m)
# derivative of J (gradient)
grad = function (x,y,th,m) {
return( 1/m * t(x) %*% (h(x, th) - y))
} # grad(x,y,th,m)
# Hessian
H = function (x,y,th,m) {
return (1/m * t(x) %*% x * diag(h(x,th)) * diag(1 - h(x,th)))
} # H(x,y,th,m)
basic.mle <- function(x, y) {
j = array(0, c(10,1))
m = length(x$test1)
x = matrix(c(rep(1, m), x$test1, x$test2), ncol=3)
y_calc = matrix(y$admitted, ncol=1)
th = matrix(0,3)
for (i in 1:100) {
j[i] = J(x, y, th, m) # stores each iteration Cost
th = th - solve(H(x,y_calc,th,m)) %*% grad(x,y_calc,th,m)
}
j
}
j <- basic.mle(exam_x, exam_y)
plot(j, xlab="iterations", ylab="cost J")
plot(j, xlab="iterations", ylab="cost J", type = "l")
for (i in 1:100 {
for (i in 1:10) {
j[i] = J(x, y, th, m) # stores each iteration Cost
th = th - solve(H(x,y_calc,th,m)) %*% grad(x,y_calc,th,m)
}
# sigmoid
g = function (z) {
return (1 / (1 + exp(-z) ))
} # plot(g(c(1,2,3,4,5,6)))
# hypothesis
h = function (x,th) {
return( g(x %*% th) )
} # h(x,th)
# cost
J = function (x,y,th,m) {
return( 1/m * sum(-y * log(h(x,th)) - (1 - y) * log(1 - h(x,th))) )
} # J(x,y,th,m)
# derivative of J (gradient)
grad = function (x,y,th,m) {
return( 1/m * t(x) %*% (h(x, th) - y))
} # grad(x,y,th,m)
# Hessian
H = function (x,y,th,m) {
return (1/m * t(x) %*% x * diag(h(x,th)) * diag(1 - h(x,th)))
} # H(x,y,th,m)
basic.mle <- function(x, y) {
j = array(0, c(10,1))
m = length(x$test1)
x = matrix(c(rep(1, m), x$test1, x$test2), ncol=3)
y_calc = matrix(y$admitted, ncol=1)
th = matrix(0,3)
for (i in 1:10) {
j[i] = J(x, y, th, m) # stores each iteration Cost
th = th - solve(H(x,y_calc,th,m)) %*% grad(x,y_calc,th,m)
}
j
}
j <- basic.mle(exam_x, exam_y)
plot(j, xlab="iterations", ylab="cost J", type = "l")
j
j[6]
j
j = J(x, y, th, m) # stores each iteration Cost
# sigmoid
g = function (z) {
return (1 / (1 + exp(-z) ))
} # plot(g(c(1,2,3,4,5,6)))
# hypothesis
h = function (x,th) {
return( g(x %*% th) )
} # h(x,th)
# cost
J = function (x,y,th,m) {
return( 1/m * sum(-y * log(h(x,th)) - (1 - y) * log(1 - h(x,th))) )
} # J(x,y,th,m)
# derivative of J (gradient)
grad = function (x,y,th,m) {
return( 1/m * t(x) %*% (h(x, th) - y))
} # grad(x,y,th,m)
# Hessian
H = function (x,y,th,m) {
return (1/m * t(x) %*% x * diag(h(x,th)) * diag(1 - h(x,th)))
} # H(x,y,th,m)
basic.mle <- function(x, y) {
j = array(0, c(10,1))
m = length(x$test1)
x = matrix(c(rep(1, m), x$test1, x$test2), ncol=3)
y_calc = matrix(y$admitted, ncol=1)
th = matrix(0,3)
for (i in 1:10) {
j = J(x, y, th, m) # stores each iteration Cost
th = th - solve(H(x,y_calc,th,m)) %*% grad(x,y_calc,th,m)
}
j
}
j <- basic.mle(exam_x, exam_y)
# plot(j, xlab="iterations", ylab="cost J", type = "l")
j
# sigmoid
g = function (z) {
return (1 / (1 + exp(-z) ))
} # plot(g(c(1,2,3,4,5,6)))
# hypothesis
h = function (x,th) {
return( g(x %*% th) )
} # h(x,th)
# cost
J = function (x,y,th,m) {
return( 1/m * sum(-y * log(h(x,th)) - (1 - y) * log(1 - h(x,th))) )
} # J(x,y,th,m)
# derivative of J (gradient)
grad = function (x,y,th,m) {
return( 1/m * t(x) %*% (h(x, th) - y))
} # grad(x,y,th,m)
# Hessian
H = function (x,y,th,m) {
return (1/m * t(x) %*% x * diag(h(x,th)) * diag(1 - h(x,th)))
} # H(x,y,th,m)
basic.mle <- function(x, y) {
j = array(0, c(10,1))
m = length(x$test1)
# eliminate matrix, use for to see what happens to the dataframe
x = matrix(c(rep(1, m), x$test1, x$test2), ncol=3)
y_calc = matrix(y$admitted, ncol=1)
th = matrix(0,3)
for (i in 1:10) {
#update j at every iteration and keep only the last j calculated
j = J(x, y, th, m) # stores each iteration Cost
th = th - solve(H(x,y_calc,th,m)) %*% grad(x,y_calc,th,m)
}
th
}
theta <- basic.mle(exam_x, exam_y)
# plot(j, xlab="iterations", ylab="cost J", type = "l")
theta
y_test <- theta[1] + theta[2] * 20 + theta[3] * 80
y_test
basic.cv <- function(dataset, var) {
n_folds = 4
dataset_copy <- dataset
fold_size <- as.numeric(nrow(dataset) / n_folds)
for (i in 1:n_folds) {
fold = 0
while(fold < fold_size) {
index <- sample(1:nrow(dataset), 1)
fold_r <- dataset[index, ]
fold = fold + 1
}
dataset_split <- rbind(dataset_split, fold_r)
dataset_split
}
}
x <- basic.cv(exam_df, 1)
basic.cv <- function(dataset, var) {
n_folds = 4
dataset_copy <- dataset
fold_size <- as.numeric(nrow(dataset) / n_folds)
dataset_split <- data.frame(test1 = numeric(),
test2 = numeric(),
admitted = logical())
for (i in 1:n_folds) {
fold = 0
while(fold < fold_size) {
index <- sample(1:nrow(dataset), 1)
fold_r <- dataset[index, ]
fold = fold + 1
}
dataset_split <- rbind(dataset_split, fold_r)
dataset_split
}
}
x <- basic.cv(exam_df, 1)
x
browser()
