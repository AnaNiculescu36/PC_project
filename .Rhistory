cv <- Lapply(colnames(sample_df_x), FUN = basic.cv, sample = sample, y_var = y_var)
cv <- lapply(colnames(sample_df_x), FUN = basic.cv, sample = sample, y_var = y_var)
cv
cv <- vapply(colnames(sample_df_x), FUN = basic.cv, sample = sample, y_var = y_var)
cv <- sapply(colnames(sample_df_x), FUN = basic.cv, sample = sample, y_var = y_var)
cv
source('C:/Users/anama/Desktop/ciorna.R', echo=TRUE)
source('C:/Users/anama/Desktop/ciorna.R', echo=TRUE)
source('C:/Users/anama/Desktop/ciorna.R', echo=TRUE)
source('C:/Users/anama/Desktop/ciorna.R', echo=TRUE)
source('C:/Users/anama/Desktop/ciorna.R', echo=TRUE)
install.packages("ezknitr")
library(ezknitr)
ezknit(file = "Niculescu_Ana_PC_homework.Rmd")
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(ggplot2)
exam_x = read.delim("data/ex4x.dat", sep = " ")
exam_y = read.delim("data/ex4y.dat")
# cleaning the data
exam_x <- exam_x[ ,c(4, 7)]
colnames(exam_x) <- c("test1", "test2")
colnames(exam_y) <- "admitted"
exam_df <- dplyr::bind_cols(exam_x, exam_y)
ggplot2::ggplot(exam_df, ggplot2::aes(x = test1, y = test2, col = as.factor(admitted))) +
ggplot2::geom_point()
g = function (z) {
return (1 / (1 + exp(-z) ))
} # plot(g(c(1,2,3,4,5,6)))
# hypothesis
h = function (x,th) {
return( g(x %*% th) )
} # h(x,th)
# cost
J = function (x,y,th,m) {
return( 1/m * sum(-y * log(h(x,th)) - (1 - y) * log(1 - h(x,th))) )
} # J(x,y,th,m)
# derivative of J (gradient)
grad = function (x,y,th,m) {
return( 1/m * t(x) %*% (h(x, th) - y))
} # grad(x,y,th,m)
# Hessian
H = function (x,y,th,m) {
return (1/m * t(x) %*% x * diag(h(x,th)) * diag(1 - h(x,th)))
} # H(x,y,th,m)
basic.mle <- function(x, y) {
j = array(0, c(10,1))
m = length(y)
# eliminate matrix, use for to see what happens to the dataframe
if (is.vector(x)) {
x_copy <- dplyr::data_frame(x_var = x,
id = rep(1, length(x)))
} else {
x_copy <- x
x_copy$id <- 1
}
x = as.matrix(x_copy)
y_calc = matrix(y, ncol = 1)
th = matrix(0, ncol(x))
for (i in 1:10) {
#update j at every iteration and keep only the last j calculated
j = J(x, y, th, m) # stores each iteration Cost
th = th - solve(H(x, y_calc, th, m)) %*% grad(x, y_calc, th, m)
}
th
}
theta <- basic.mle(exam_x, exam_y$admitted)
# -16.38
# 0.1483
# 0.1589
# Model = theta[1] + theta[2] * x$test1 + theta[3] * x$test_2
x_test <- c(20, 80, 1)
y_test <- t(x_test) %*% theta
# the function returrns the cv error for each fold
basic.cv <- function(sample, x_vars, y_var) {
n_folds = 10
set.seed(1)
sample_copy <- sample[sample(nrow(sample)),]
fold_size <- as.numeric(nrow(sample) / n_folds)
folds <- c()
for (fold in 1:n_folds) {
fold_r = c()
n <- 0
while(n < fold_size) {
fold_r <- c(fold_r, fold)
n <- n + 1
}
folds <- c(folds, fold_r)
}
error_c <- c()
for (i in 1:n_folds) {
#Segement data by fold
test_indexes <- which(folds == i, arr.ind = TRUE)
test_data <-  test_data <- cbind(sample_copy[test_indexes, ],
data.frame(intercept = rep(1, length(test_indexes))))
train_data <- sample_copy[-test_indexes, ]
# train model using basic.mle
thetas <- basic.mle(train_data[,x_vars], train_data[[y_var]])
x_vars2 <- c(x_vars, "intercept")
# use thetas to predict the model in test_data
predicted_y <- c()
for (k in 1:nrow(test_data)) {
mat_test <- as.matrix(test_data[k, x_vars2])
sum <- 0
for(l in 1:length(thetas)) {
sum <- sum + mat_test[l] * thetas[l]
}
predicted_y[k] <- sum
#  predicted_y[k] <- mat_test %*% thetas
predicted_y[k] <- ifelse(predicted_y[k] > 0.5, 1, 0)
}
# calculate error
wrong <- (predicted_y != test_data[[y_var]])
count_wrong <- sum(wrong)
error_c[i] <- count_wrong/nrow(test_data)
}
mean(error_c)
}
exam_df_c <- exam_df %>%
dplyr::mutate(test3 = test2 ^ 2)
x <- basic.cv(exam_df_c, c("test1", "test2", "test3"), "admitted")
x2 <- basic.cv(exam_df_c, c("test1", "test2"), "admitted")
x3 <-  basic.cv(exam_df_c, c("test1"), "admitted")
# models argument is a list containing models as a form of named lists containing two vectors:
# predictors and predicted
# models argument is a list containing models as a form of named lists containing two vectors:
# predictors and predicted
model1 <-
list(
predictors = c("test1"),
predicted = c("admitted"))
model2 <-
list(
predictors = c("test1", "test2"),
predicted = c("admitted"))
models_list <- list()
models_list[[1]] <- model1
models_list[[2]] <- model2
# Better: purr parr aplly on list, calc cv for each element, which.min error
basic.modelcomparison <- function(sample, models) {
if (length(models) < 2) {
stop("models list should contain at least two models!")
}
# initialise cv error with a very big value in order to calculate minimum
min_cv <- 100
for (i in 1:length(models)) {
cv_error <- basic.cv(
sample,
models[[i]]$predictors,
models[[i]]$predicted)
if (cv_error < min_cv) {
min_cv <- cv_error
index <- i
}
}
best_cv <- list(
best_model = models[[index]],
cv_error = min_cv)
return(best_cv)
}
# add another variable to the dataset to illustrate the way function
# works
exam_df_c <- exam_df %>%
dplyr::mutate(test3 = test2 ^ 2)
best <- basic.modelcomparison(exam_df_c, models_list)
basic.modelselection <- function(sample, method, y_var) {
sample_df_x <- exam_df_c[,!(names(exam_df_c) == y_var)]
all_vars <- list(predictors = colnames(sample_df_x),
predicted = c(y_var))
# start with an empyt model for forward selection,
# we decide which is the initial variable
# included in the model based on min cv error
cv <- c()
names_of_df <- colnames(sample_df_x)
for (i in 1:ncol(sample_df_x)) {
cv[i] <- basic.cv(sample, names_of_df[i], y_var)
}
min_cv_idx <- which.min(cv)
included <- list(
predictors = c(names_of_df[min_cv_idx]),
predicted = c(y_var)
)
while (TRUE) {
changed <- FALSE
excluded <- setdiff(all_vars$predictors, included$predictors)
# forward step
best_cv <- c()
best_model <- list()
for (i in 1:length(excluded)) {
model_predictors <- c(included$predictors, excluded[i])
model <- list(predictors = model_predictors,
predicted = c(y_var))
## creare model, calcul
final_model <- list()
anterior_model <- included
new_model <- model
final_model[[1]] <- anterior_model
final_model[[2]] <- new_model
model_comparison <- basic.modelcomparison(sample, final_model)
best_cv[i] <- model_comparison$cv_error
best_model[[i]] <- model_comparison$best_model
}
min_cv_index <- which.min(best_cv)
comparing_array <-
best_model[[min_cv_index]]$predictors == included$predictors
if (all(comparing_array == TRUE)) {
changed <- FALSE
bm$predictors <- included$predictors
bm$predicted <- included$predicted
cv2 <- cv[which.min(cv)]
res <- list(best_model = bm,
cv_error = cv2)
} else {
bm <- list()
included <- best_model[[min_cv_index]]
cv <- best_cv[[min_cv_index]]
bm$predictors <- included$predictors
bm$predicted <- included$predicted
res <- list(best_model = bm,
cv_error = cv)
changed <- TRUE
}
if (changed == FALSE) {
break
}
}
res
}
basic.modelselection(exam_df_c, "forward", "admitted")
source('C:/Users/anama/Desktop/ciorna.R', echo=TRUE)
library(microbenchmark)
source('C:/Users/anama/Desktop/ciorna.R', echo=TRUE)
source('C:/Users/anama/Desktop/ciorna.R', echo=TRUE)
best_model2
best_model[[min_cv_index]]
best_model2[[min_cv_index]]
best_model2[[min_cv_index]]$best_model
source('C:/Users/anama/Desktop/ciorna.R', echo=TRUE)
source('C:/Users/anama/Desktop/ciorna.R', echo=TRUE)
bm1
bm
source('C:/Users/anama/Desktop/ciorna.R', echo=TRUE)
source('C:/Users/anama/Desktop/ciorna.R', echo=TRUE)
bm1
bm
source('C:/Users/anama/Desktop/ciorna.R', echo=TRUE)
min_cv_idx
best_model2
unname(sapply(best_model2, `[[`, 2))
best_model2[[min_cv_index]]$predictors == included$predictors
best_model2[[min_cv_index]]$predictors
best_model2
best_model2[[1]]
best_model2[[1]]$predictors
best_model2[[1]]$best_model$predictors
source('C:/Users/anama/Desktop/ciorna.R', echo=TRUE)
bm1
bm
source('C:/Users/anama/Desktop/ciorna.R', echo=TRUE)
source('C:/Users/anama/Desktop/ciorna.R', echo=TRUE)
source('C:/Users/anama/Desktop/ciorna.R', echo=TRUE)
source('C:/Users/anama/Desktop/ciorna.R', echo=TRUE)
source('C:/Users/anama/Desktop/ciorna.R', echo=TRUE)
source('C:/Users/anama/Desktop/ciorna.R', echo=TRUE)
source('C:/Users/anama/Desktop/ciorna.R', echo=TRUE)
source('C:/Users/anama/Desktop/ciorna.R', echo=TRUE)
source('C:/Users/anama/Desktop/ciorna.R', echo=TRUE)
library(aprof)
dump("basic.modelselection1", file="basic_modelselection.R")
source("basic_modelselection.R")
Rprof("monprofil4.Rout",
line.profiling = TRUE,
interval = 0.01)
basic.modelselection(exam_df_c, "forward", "admitted")
Rprof(append=FALSE)
basic_ms_aprof <- aprof("basic_modelselection.R", "monprofil4.Rout")
plot(basic_ms_aprof)
Rprof(NULL)
file.remove("Rprof.out")
file.remove("monprofil4.Rout")
file.remove("basic_modelselection.R")
dump("basic.modelselection1", file="basic_modelselection.R")
source("basic_modelselection.R")
Rprof("monprofil4.Rout",
line.profiling = TRUE,
interval = 0.01)
basic.modelselection(exam_df_c, "forward", "admitted")
Rprof(append=FALSE)
basic_ms_aprof <- aprof("basic_modelselection.R", "monprofil4.Rout")
plot(basic_ms_aprof)
Rprof(NULL)
file.remove("Rprof.out")
file.remove("monprofil4.Rout")
file.remove("basic_modelselection.R")
dump("basic.modelselection1", file="basic.modelselection1.R")
source("basic_modelselection.R")
Rprof("monprofil4.Rout",
line.profiling = TRUE,
interval = 0.01)
basic.modelselection(exam_df_c, "forward", "admitted")
Rprof(append=FALSE)
basic_ms_aprof <- aprof("basic.modelselection1.R", "monprofil4.Rout")
plot(basic_ms_aprof)
Rprof(NULL)
file.remove("Rprof.out")
file.remove("monprofil4.Rout")
file.remove("basic.odelselection1.R")
dump("basic.modelselection1", file="basic.modelselection1.R")
source("basic_modelselection.R")
Rprof("monprofil4.Rout",
line.profiling = TRUE,
interval = 0.01)
basic.modelselection(exam_df_c, "forward", "admitted")
Rprof(append=FALSE)
basic_ms_aprof <- aprof("basic.modelselection1.R", "monprofil4.Rout")
plot(basic_ms_aprof)
Rprof(NULL)
file.remove("Rprof.out")
file.remove("monprofil4.Rout")
file.remove("basic.modelselection1.R")
dump("basic.modelselection1", file="basic.modelselection1.R")
source("basic_modelselection.R")
Rprof("monprofil4.Rout",
line.profiling = TRUE,
interval = 0.01)
basic.modelselection(exam_df_c, "forward", "admitted")
Rprof(append=FALSE)
basic_ms_aprof <- aprof("basic.modelselection1.R", "monprofil4.Rout")
plot(basic_ms_aprof)
Rprof(NULL)
file.remove("Rprof.out")
file.remove("monprofil4.Rout")
file.remove("basic.modelselection1.R")
dump("basic.modelselection1", file="basic.modelselection1.R")
source("basic.modelselection1.R")
Rprof("monprofil4.Rout",
line.profiling = TRUE,
interval = 0.01)
basic.modelselection(exam_df_c, "forward", "admitted")
Rprof(append=FALSE)
basic_ms_aprof <- aprof("basic.modelselection1.R", "monprofil4.Rout")
plot(basic_ms_aprof)
Rprof(NULL)
file.remove("Rprof.out")
file.remove("monprofil4.Rout")
file.remove("basic.modelselection1.R")
dump("basic.modelselection1", file="basic.modelselection1.R")
source("basic.modelselection1.R")
Rprof("monprofil4.Rout",
line.profiling = TRUE,
interval = 0.01)
basic.modelselection(exam_df_c, "forward", "admitted")
Rprof(append=FALSE)
basic_ms_aprof <- aprof(" basic.modelselection1.R", "monprofil4.Rout")
plot(basic_ms_aprof)
Rprof(NULL)
file.remove("Rprof.out")
file.remove("monprofil4.Rout")
file.remove("basic.modelselection1.R")
out <- microbenchmark(basic.modelselection(exam_df_c, "forward", "admitted"), unit = "ms")
boxplot(out)
dump("basic.modelselection", file="basic_modelselection.R")
source("basic_modelselection.R")
Rprof("monprofil4.Rout",
line.profiling = TRUE,
interval = 0.01)
basic.modelselection(exam_df_c, "forward", "admitted")
Rprof(append=FALSE)
basic_ms_aprof <- aprof("basic_modelselection.R", "monprofil4.Rout")
plot(basic_ms_aprof)
Rprof(NULL)
file.remove("Rprof.out")
file.remove("monprofil4.Rout")
file.remove("basic_modelselection.R")
# For the second approach, instead of looping through each
# row of the dataframe, transforming it into a matrix at each
# iteration (which is time consuming, as seen with aprof)
# and calculated the dot product, we directly multply the
# matrix to thetas (vectorial product)
basic.cv2 <- function(sample, x_vars, y_var) {
n_folds = 10
set.seed(1)
sample_copy <- sample[sample(nrow(sample)), ]
folds <- cut(seq(1, nrow(sample_copy)), breaks = 10, labels = FALSE)
error_c <- numeric(n_folds)
for (i in 1:n_folds) {
#Segement data by fold
test_indexes <- which(folds == i, arr.ind = TRUE)
test_data <-  sample_copy[test_indexes, ]
train_data <- sample_copy[-test_indexes, ]
# train model using basic.mle
thetas <- basic.mle(train_data[,x_vars], train_data[,y_var])
test_data$intercept <- 1
x_vars2 <- c(x_vars, "intercept")
# use thetas to predict the model in test_data
predicted_y <- numeric(nrow(test_data))
mat_test <- as.matrix(test_data[, x_vars2])
predicted_y <- ifelse(mat_test %*% thetas > 0.5, 1, 0)
# calculate error
wrong <- (predicted_y != test_data[[y_var]])
count_wrong <- sum(wrong)
error_c[i] <- count_wrong/nrow(test_data)
}
mean(error_c)
}
out <- microbenchmark(basic.modelselection(exam_df_c, "forward", "admitted"), unit = "ms")
boxplot(out)
dump("basic.modelselection", file="basic_modelselection.R")
source("basic_modelselection.R")
Rprof("monprofil4.Rout",
line.profiling = TRUE,
interval = 0.01)
basic.modelselection(exam_df_c, "forward", "admitted")
Rprof(append=FALSE)
basic_ms_aprof <- aprof("basic_modelselection.R", "monprofil4.Rout")
plot(basic_ms_aprof)
Rprof(NULL)
file.remove("Rprof.out")
file.remove("monprofil4.Rout")
file.remove("basic_modelselection.R")
source('C:/Users/anama/Desktop/ciorna.R', echo=TRUE)
source('C:/Users/anama/Desktop/ciorna.R', echo=TRUE)
source('C:/Users/anama/Desktop/ciorna.R', echo=TRUE)
source('C:/Users/anama/Desktop/ciorna.R', echo=TRUE)
# Better: purr parr aplly on list, calc cv for each element, which.min error
basic.modelcomparison1 <- function(sample, models) {
if (length(models) < 2) {
stop("models list should contain at least two models!")
}
cv_error <- numeric(length(models))
for (i in 1:length(models)) {
cv_error[i] <- basic.cv(
sample,
models[[i]]$predictors,
models[[i]]$predicted)
}
index <- which.min(cv_error)
return(list(
best_model = models[[index]],
cv_error = cv_error[index]))
}
no_cores <- detectCores() - 1
# Better: purr parr aplly on list, calc cv for each element, which.min error
basic.modelcomparison1 <- function(sample, models) {
if (length(models) < 2) {
stop("models list should contain at least two models!")
}
cv_error <- numeric(length(models))
for (i in 1:length(models)) {
cv_error[i] <- basic.cv2(
sample,
models[[i]]$predictors,
models[[i]]$predicted)
}
index <- which.min(cv_error)
return(list(
best_model = models[[index]],
cv_error = cv_error[index]))
}
no_cores <- detectCores() - 1
source('C:/Users/anama/Desktop/ciorna.R', echo=TRUE)
source('C:/Users/anama/Desktop/ciorna.R', echo=TRUE)
source('C:/Users/anama/Desktop/ciorna.R', echo=TRUE)
install.packages("foreach")
library(foreach)
source('C:/Users/anama/Desktop/ciorna.R', echo=TRUE)
source('C:/Users/anama/Desktop/ciorna.R', echo=TRUE)
source('C:/Users/anama/Desktop/ciorna.R', echo=TRUE)
bm <- basic.modelselection(exam_df_c, "forward", "admitted")
bm1 <- basic.modelselection1(exam_df_c, "forward", "admitted")
all.equal(bm, bm1)
bm <- basic.modelselection(exam_df_c, "forward", "admitted")
bm2 <- basic.modelselection2(exam_df_c, "forward", "admitted")
all.equal(bm, bm2)
source('C:/Users/anama/Desktop/ciorna.R', echo=TRUE)
# For the second approach, instead of looping through each
# row of the dataframe, transforming it into a matrix at each
# iteration (which is time consuming, as seen with aprof)
# and calculated the dot product, we directly multply the
# matrix to thetas (vectorial product)
basic.cv2 <- function(sample, x_vars, y_var) {
n_folds = 10
set.seed(1)
sample_copy <- sample[sample(nrow(sample)), ]
folds <- cut(seq(1, nrow(sample_copy)), breaks = 10, labels = FALSE)
error_c <- numeric(n_folds)
for (i in 1:n_folds) {
#Segement data by fold
test_indexes <- which(folds == i, arr.ind = TRUE)
test_data <-  sample_copy[test_indexes, ]
train_data <- sample_copy[-test_indexes, ]
# train model using basic.mle
thetas <- basic.mle(train_data[,x_vars], train_data[,y_var])
test_data$intercept <- 1
x_vars2 <- c(x_vars, "intercept")
# use thetas to predict the model in test_data
predicted_y <- numeric(nrow(test_data))
mat_test <- as.matrix(test_data[, x_vars2])
predicted_y <- ifelse(mat_test %*% thetas > 0.5, 1, 0)
# calculate error
wrong <- (predicted_y != test_data[[y_var]])
count_wrong <- sum(wrong)
error_c[i] <- count_wrong/nrow(test_data)
}
mean(error_c)
}
source('C:/Users/anama/Desktop/ciorna.R', echo=TRUE)
source('C:/Users/anama/Desktop/ciorna.R', echo=TRUE)
source('C:/Users/anama/Desktop/ciorna.R', echo=TRUE)
source('C:/Users/anama/Desktop/ciorna.R', echo=TRUE)
source('C:/Users/anama/Desktop/ciorna.R', echo=TRUE)
ezknitr::ezknit("Niculescu_Ana_PC_homework.Rmd")
