---
title: "Parallel computing report"
author: "Niculescu Ana-Maria"
date: "December 22, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(dplyr)
library(ggplot2)
```

## Purpose
Using Newton’s Method to implement logistic regression on a classification problem.

## Chosen dataset
In order to test the implementation of the required functions for this project, the following dataset has been chosen: 
40 students who were admitted to college and 40 students who were not admitted, and their corresponding grades for 2 exams.

```{r}
exam_x = read.delim("data/ex4x.dat", sep = " ")
exam_y = read.delim("data/ex4y.dat")

# cleaning the data 
exam_x <- exam_x[ ,c(4, 7)]
colnames(exam_x) <- c("test1", "test2")
colnames(exam_y) <- "admitted"
exam_df <- dplyr::bind_cols(exam_x, exam_y)
```

Plotting the data
```{r}
ggplot2::ggplot(exam_df, ggplot2::aes(x = test1, y = test2, col = as.factor(admitted))) +
  ggplot2::geom_point()
```


###  1. Maximum log-likelihood using Newton-Raphson algorithm 
basic.mle function takes as input argument the subset of the relevant variables and the variable to predict. This function returns the maximum likelihood estimator. This estimator is given by a Newton-Raphson algorithm.
Newton’s method, similarly to gradient descent, is a way to search for the 0(minimum) of the derivative of the cost function.
```{r}
# sigmoid
g = function (z) {
  return (1 / (1 + exp(-z) ))
} # plot(g(c(1,2,3,4,5,6)))

# hypothesis 
h = function (x,th) {
  return( g(x %*% th) )
} # h(x,th)

# cost
J = function (x,y,th,m) {
  return( 1/m * sum(-y * log(h(x,th)) - (1 - y) * log(1 - h(x,th))) )
} # J(x,y,th,m)

# derivative of J (gradient)
grad = function (x,y,th,m) {
  return( 1/m * t(x) %*% (h(x, th) - y))
} # grad(x,y,th,m)

# Hessian
H = function (x,y,th,m) {
  return (1/m * t(x) %*% x * diag(h(x,th)) * diag(1 - h(x,th)))
} # H(x,y,th,m)

basic.mle <- function(x, y) {
  j = array(0, c(10,1))
  m = length(x$test1)
  # eliminate matrix, use for to see what happens to the dataframe
  x = matrix(c(rep(1, m), x$test1, x$test2), ncol=3)
  y_calc = matrix(y$admitted, ncol=1)
  th = matrix(0,3)
  for (i in 1:10) {
     #update j at every iteration and keep only the last j calculated
      j = J(x, y, th, m) # stores each iteration Cost
      th = th - solve(H(x,y_calc,th,m)) %*% grad(x,y_calc,th,m) 
  }
  th
}

theta <- basic.mle(exam_x, exam_y)

# -16.38
# 0.1483
# 0.1589
# Model = theta[1] + theta[2] * x$test1 + theta[3] * x$test_2
y_test <- theta[1] + theta[2] * 20 + theta[3] * 80
```


```{r}
basic.cv2 <- function(dataset, var) {
  n_folds = 10
  dataset_copy <- dataset
  fold_size <- as.numeric(nrow(dataset) / n_folds)
  folds <- c()
	for (fold in 1:n_folds) { 
	  fold_r = c()
	  n <- 0
	  while(n < fold_size) {
	    fold_r <- c(fold_r, fold)
	    n <- n + 1
	  }
	 folds <- c(folds, fold_r)
	}
  
  for(i in 1:10){
    #Segement your data by fold using the which() function 
    test_indexes <- which(folds==i, arr.ind=TRUE)
    test_data <- dataset[test_indexes, ]
    train_data <- dataset[-test_indexes, ]
    # train model using basic.mle
    thetas <- basic.mle(train_data[, c("test1", "test2"), ], train_data$admitted)
    # use thetas to predict the model in test_data
    predicted_admitted <- thetas[1] + thetas[2] * test_data$test1 + thetas[3] * test_data$test2
    predicted_admitted <- ifelse(predicted_admitted > 0.5, 1, 0)
    # test prediction
    
  }
}

x <- basic.cv(exam_df, 1)
```


```{r}
basic.modelcomparison <- function() {
  
}
```


```{r}
basic.modelselection <- function() {
  
}
```


### Testing methods 

```{r}

```


### Code monitoring
```{r}

```


### Alternatives

```{r}
mle <- function(x, y) {
  # add intercept term to predictors
  x$intercept = 1
  # initialise fitting parameters
  theta = numeric(3)
  # sigmoid function
  g = 
  for (i in 1:100) {
    
  }
}
```


```{r}
basic.cv <- function(dataset, var) {
  #Randomly shuffle the data
  dataset <- dataset[sample(nrow(dataset)),]
  
  #Create 10 equally size folds
  folds <- cut(seq(1, nrow(dataset)), breaks=10, labels=FALSE)
   #Perform 10 fold cross validation
  for(i in 1:10){
    #Segement your data by fold using the which() function 
    test_indexes <- which(folds==i, arr.ind=TRUE)
    test_data <- dataset[testIndexes, ]
    train_data <- dataset[-testIndexes, ]
    #Use the test and train data partitions however you desire...
  }
 folds
}

y <- basic.cv(exam_df)
 
cv <- function(sample, var) {
  
}
```


```{r}
modelcomparison <- function() {
  
}
```


```{r}
modelselection <- function() {
  
}
```

### Illustrate the gain


```{r}

```


### Testing methods 

```{r}

```


```{r}

```